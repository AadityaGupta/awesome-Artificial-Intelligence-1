{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropogation\n",
    "\n",
    " Backpropagation is a powerful tool that allows the ANN to learn the knowledge of the input vectors without having to write any specific rules to encode the knowledge. \n",
    " <br>\n",
    " Feeding back of the rate of change of error with respect to the weight is what is called as backpropagation.\n",
    "<br>\n",
    "![alt text](backP.gif \"Can't understand it\")\n",
    "\n",
    "\n",
    "### Let's start from scratch\n",
    "\n",
    "![](backP1.png \"Back Propogation\")\n",
    "\n",
    "\n",
    "Backpropogation essentially means, that you look at how wrong the network guessed, and then adjust the networks weights accordingly through backpropogating.\n",
    "<br>\n",
    "This process happens backwards, because you start at the end of the network (observe how wrong the networks ‘guess’ is), and then move backwards through the network, while adjusting the weights on the way, until you finally reach the inputs.\n",
    "\n",
    "To calculate this by hand requires some calculus which you have already studied in 12th std., as it involves getting some derivatives of the networks’ weights\n",
    "<br>\n",
    "In a multi-layered ANN, every unit of neuron affects many output units in the next layer. Let’s consider the illustration below.\n",
    "\n",
    "\n",
    "![alt text](backP2.png \"Back Propogation\")\n",
    "\n",
    "Here, each hidden activity in the layer ‘i’ can affect many outputs in the layer ‘j’ and hence shall have many different errors. It is prudent to combine these errors. The idea behind combining the different errors is to compute the rate of change of error for all the units at the same time for every iteration.\n",
    "The rate of change is nothing but the error derivative of the units.\n",
    "\n",
    "So we are trying to find a way to reduce the error to zero in every step of the iteration.\n",
    "\n",
    "![alt text](backPGD.gif \"Gradiend Descent Back Propogation\")\n",
    "\n",
    "**Fundamental Math behind Backpropagation :**<br><br>\n",
    "In order to truly understand backpropagation, we need to understand 2 simple truths.\n",
    "- We cannot directly arrive at the rate of change of the error with respect to weights for all the units. Instead, we need to first compute the rate of change of error with respect to the activation functions of the hidden activities.\n",
    "- Once the rate of change of error with respect to the activation function is known, then using chain-rule, we can compute the rate of change of error with respect to the hidden units.\n",
    "\n",
    "![alt text](backP4.png \"Back Propogation\")\n",
    "\n",
    "\n",
    "### Derivation :\n",
    "For a neuron j with activation function g(x), change in weight is given by :<br>\n",
    "\n",
    "![alt text](deltaEq.png \"Delta Equation Propogation\")\n",
    "\n",
    "It holds that h(j) :\n",
    "\n",
    "![alt text](hj.png \"H(J) EQUATION\")\n",
    "\n",
    "The delta rule is derived by attempting to minimize the error in the output of the neural network through gradient descent. The error for a neural network with \n",
    "\n",
    "\n",
    "\n",
    "![alt text](Eeq.png \"Delta Equation Propogation\")\n",
    "\n",
    "![alt text](wholeDerivation.png \"Delta Equation Propogation\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![alt text](backPAlgo.gif \"Back Propogation Algo\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Algorithm \n",
    "\n",
    "Here are the steps for applying backpropogation :\n",
    "- Calculate net, which is summation of products of x(i) and W(ij).\n",
    "- Calculate f(net) by bipolar sigmoid activation function.\n",
    "- Compare f(net) with desired output and check if they are equal. \n",
    "- If they are not equal :\n",
    "- - Calculate derivation of f(net), which can be calculated as 1/2(1-f(net)^2)\n",
    "- - Calculate change in weight by the formulae given above.\n",
    "- Else:\n",
    "- - No need to update the weights.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
