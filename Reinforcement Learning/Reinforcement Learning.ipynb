{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning \n",
    "\n",
    "Reinforcement Learning is the science of making optimal decisions. It helps us formulate **reward-motivated** behaviour exhibited by living species . \n",
    "<br><br>\n",
    "Lets say, you want to make a kid sit down to study for an exam. It is very difficult to do so, but if you give him a bar of chocolate every time he finishes a chapter/topic he will understand that if he keeps on studying he will get more chocolate bars. So he will have some motivation to study for the exam. <br>\n",
    "Now initially the kid has no sense of time or how to prepare(he might go through every line and ponder upon it). He might take up hours studying a topic and never finish the syllabus in time. So, lets say if he finishes a topic within an hour we give him a huge bar, if he takes 1.5 hours a regular bar and a toffee if he takes longer than that. So, now not only does he study but his brain devises ways in which he can finish topics faster.\n",
    "\n",
    "<br>\n",
    "The kid represents the Agent . The reward system and the exam represent the Environment. The topics are analogous to States in reinforcement learning. So, the kid has to decide which topics to give more importance to(i.e., to calculate the value of each topic). This will be the work of our Value-Function . So, every time he travels from one state to another he gets a Reward and the methods he uses to complete the topics within time is our Policy.\n",
    "\n",
    "**Something you must know :**\n",
    "This is carried out by our Dopamine system in our brain which takes care of reward-motivated behaviour. Most types of rewards increase the level of dopamine in the brain. For example, when you take addictive drugs it increases dopamine neuronal activity. So you feel like you are rewarded and you feel satisfied, hence the effect.\n",
    "\n",
    "### How is it different from other machine learning paradigms :\n",
    "- **There is no supervisor present**. So, there is no one to tell you the best possible action. You just get a reward for each action.\n",
    "- **Importance of time**. Reinforcement Learning pays much attention to sequential data unlike other paradigms where you receive random inputs. Here the input at the next step will always be dependent on input at previous state.\n",
    "- **Concept of delayed rewards**. You may not get a reward at each step. A reward may be given only after the completion of the entire task. Also, say you get a reward at a step only to find out that you made a huge blunder in a future step.\n",
    "- **The agents action effects its next input**. Say, you have the choice of going either left or right. After you take the action, the input at the next time step will be different if you chose right rather than left.\n",
    "\n",
    "\n",
    "![alt text](reinforcement.png \"Reinforcement learning\")\n",
    "\n",
    "\n",
    "So, at any time-step (t), our agent sees an observation(instance) of the environment. Then out of the actions possible it chooses an action for which the environment gives it a reward and the next observation. So we will provide algorithms to our agent to take decisions with the goal to maximize our overall reward at the end.\n",
    "\n",
    "\n",
    "\n",
    "**Some common games we all have played!**\n",
    "<br>\n",
    "\n",
    "| Image1  | Image 2 |\n",
    "|:---:|:---:|\n",
    "| ![alt text](gameRein.gif \"Reinforcement learning game1\") | ![alt text](gameRein2.png \"Reinforcement learning game2\") | \n",
    "\n",
    "\n",
    "\n",
    "### Q Learning :\n",
    "**Let's learn by an example**\n",
    "You start at a given position, the starting state . From any state you can go left, right, up or down or stay in the same place provided you don’t cross the premises of the maze. Each action will take you to a cell of the grid (a different state). Now, there is a treasure chest at one of the states (the goal state). Also, the maze has a pit of snakes in certain positions/states. Your goal is to travel from the starting state to the goal state by following a path that doesn’t have snakes in it.\n",
    "<br>\n",
    "![alt text](snake.png \"Snake Reinforcement learning\")\n",
    "<br>\n",
    "**Q-Learning attempts to learn the value of being in a given state, and taking a specific action there.**\n",
    "\n",
    "We start by initializing the table to be uniform (all zeros), and then as we observe the rewards we obtain for various actions, we update the table accordingly. We will update the table using the Bellman Equation\n",
    "<br>\n",
    "![alt text](equation.png \"equation Reinforcement learning\")\n",
    "<br>\n",
    "\n",
    "Lets see how to calculate the Q table :\n",
    "For this purpose we will take a smaller maze-grid for ease.\n",
    "<br><br>\n",
    "![alt text](snake2.png \"Snake Reinforcement learning\")\n",
    "<br>\n",
    "\n",
    "\n",
    "The initial Q-table would look like ( states along the rows and actions along the columns ) :\n",
    "<br>\n",
    "![alt text](0matrix.png \"Snake Reinforcement learning\")\n",
    "<br>\n",
    "The reward table would look like :\n",
    "<br>\n",
    "![alt text](reward.png \"Snake Reinforcement learning\")\n",
    "<br>\n",
    "Algorithm:\n",
    "- Initialise Q-matrix by all zeros. Set value for ‘γ’. Fill rewards matrix.\n",
    "- For each episode. Select a random starting state (here we will restrict our starting state to state-1).\n",
    "- Select one among all possible actions for the current state (S).\n",
    "- Travel to the next state (S’) as a result of that action (a).\n",
    "- For all possible actions from the state (S’) select the one with the highest Q value.\n",
    "- Update Q-table using eqn.1 .\n",
    "- Set the next state as the current state.\n",
    "- If goal state reached then end.\n",
    "\n",
    "Example : Lets say we start with state 1 . We can go either D or R. Say, we chose D . Then we will reach 3 (the snake pit). So, then we can go either U or R . So, taking γ = 0.8, we have :<br>\n",
    "\n",
    "Q(1,D) = R(1,D) + γ*[max(Q(3,U) & Q(3,R))]\n",
    "<br>\n",
    "Q(1,D) = -10 + 0.8*0 = -10\n",
    "<br>\n",
    "Here, max(Q(3,U) & Q(3,R)) = 0 as Q matrix not yet updated. \n",
    "<br> -10 is for stepping on the snakes. So, new Q-table looks like :\n",
    "<br>\n",
    "![alt text](matrix2.png \"Snake Reinforcement learning\")\n",
    "<br>\n",
    "Now, 3 is the starting state. From 3, lets say we go R. So, we go on 4 . From 4, we can go U or L .\n",
    "<br>\n",
    "Q(3,R) = R(3,R) + 0.8*[max(Q(4,U) & Q(4,L))]\n",
    "<br>\n",
    "Q(3,R) = 10 + 0.8*0 = 10\n",
    "<br>\n",
    "![alt text](finalmatrix.png \"Snake Reinforcement learning\")\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Reinforcement Learning concepts - Video** :\n",
    "\n",
    "[![Reinforcement Learning](https://img.youtube.com/vi/m2weFARriE8/0.jpg)](https://www.youtube.com/watch?time_continue=4&v=m2weFARriE8)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
