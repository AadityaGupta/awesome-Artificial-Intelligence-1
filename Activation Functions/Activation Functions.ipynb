{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions \n",
    "\n",
    "### Definition \n",
    "It’s just a thing (node) that you add to the output end of any neural network. It is also known as Transfer Function. It can also be attached in between two Neural Networks.\n",
    "\n",
    "### Why ?\n",
    "\n",
    "It is used to determine the output of neural network like yes or no. It maps the resulting values in between 0 to 1 or -1 to 1 etc. (depending upon the function).\n",
    "\n",
    "**The Activation Functions can be basically divided into 2 types-**\n",
    "- Linear Activation Functions\n",
    "- Non Linear Activation Functions\n",
    "\n",
    "### Linear or Identity Activation Function\n",
    "As you can see the function is a line or linear.Therefore, the output of the functions will not be confined between any range.\n",
    "<br>\n",
    "![alt text](linearAF.png \"Linear Activation Function\")\n",
    "\n",
    "*Equation* : f(x) = x<br>\n",
    "*Range* : (-infinity to infinity)<br>\n",
    "It doesn’t help with the complexity or various parameters of usual data that is fed to the neural networks.\n",
    "\n",
    "### Non-linear Activation Function\n",
    "\n",
    "The Nonlinear Activation Functions are the most used activation functions. Nonlinearity helps to makes the graph look something like this :\n",
    "<br>\n",
    "![alt text](nonlinearAF.png \"Non linear Activation Function\")\n",
    "\n",
    "It makes it easy for the model to generalize or adapt with variety of data and to differentiate between the output.\n",
    "<br>\n",
    "The main terminologies needed to understand for nonlinear functions are:\n",
    "\n",
    "The Nonlinear Activation Functions are mainly divided on the basis of their **range or curves-**\n",
    "\n",
    "#### 1. Sigmoid or Logistic Activation Function\n",
    "\n",
    "The Sigmoid Function curve looks like a S-shape.\n",
    "<br>\n",
    "![alt text](sigmoid.png \"Sigmoid Activation Function\")\n",
    "\n",
    "The main reason why we use sigmoid function is because it exists between (0 to 1). \n",
    "It is used to predict probability as an output.\n",
    "<br>\n",
    "*The function is differentiable.That means, we can find the slope of the sigmoid curve at any two points.\n",
    "The function is monotonic but function’s derivative is not*\n",
    "\n",
    "### Be Quick :\n",
    "> Why is it used to predict probability as an output?\n",
    "\n",
    "#### 2. Tanh or hyperbolic tangent Activation Function\n",
    "tanh is also like logistic sigmoid but better. The range of the tanh function is from (-1 to 1). tanh is also sigmoidal (s - shaped).\n",
    "<br>\n",
    "![alt text](tanh.png \"Tanh Activation Function\")\n",
    "\n",
    "The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph.\n",
    "*The function is differentiable.\n",
    "The function is monotonic while its derivative is not.*\n",
    "\n",
    "**The tanh function is mainly used classification between two classes.**\n",
    "\n",
    "#### 3. ReLU (Rectified Linear Unit) Activation Function\n",
    "The ReLU is the most used activation function in the world right now.Since, it is used in almost all the convolutional neural networks or deep learning.\n",
    "<br>\n",
    "![alt text](relu.png \"ReLu Activation Function\")\n",
    "\n",
    "*The function and its derivative both are monotonic.*\n",
    "\n",
    "**Issue with ReLu :** All the negative values become zero immediately which decreases the ability of the model to fit or train from the data properly. That means any negative input given to the ReLU activation function turns the value into zero immediately in the graph, which in turns affects the resulting graph by not mapping the negative values appropriately.\n",
    "\n",
    "#### 4. Leaky ReLU\n",
    "It is an attempt to solve the dying ReLU problem\n",
    "<br>\n",
    "![alt text](leaky.png \"Leaky Activation Function\")\n",
    "\n",
    "The leak helps to increase the range of the ReLU function.Usually, the value of a is 0.01 or so.<br>\n",
    "When a is not 0.01 then it is called Randomized ReLU.<br>\n",
    "Therefore the range of the Leaky ReLU is (-infinity to infinity).<br>\n",
    "\n",
    "*Both Leaky and Randomized ReLU functions are monotonic in nature. Also, their derivatives also monotonic in nature*\n",
    "\n",
    "**Why derivative/differentiation is used ?**\n",
    "When updating the curve, to know in which direction and how much to change or update the curve depending upon the slope.That is why we use differentiation in almost every part of Machine Learning and Deep Learning.\n",
    "<br>\n",
    "![alt text](allinone.png \"All Activation Function\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
